{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "067c6c6b",
      "metadata": {
        "id": "067c6c6b"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Table of Contents:\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Setup](#setup)\n",
        "- [Implement Transformer](#implement-transformer)\n",
        "- [Implement Embedding Layer](#implement-embedding-layer)\n",
        "- [Implement Mini GPT](#implement-mini-gpt)\n",
        "- [Prepare Data](#prepare-data)\n",
        "- [Implement Keras Callback for Generating Text](#implement-keras-callback-for-generating-text)\n",
        "- [Train the Model](#train-the-model)\n",
        "- [Assignment](#assignment)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eadac36",
      "metadata": {
        "id": "6eadac36"
      },
      "source": [
        "<ul>\n",
        "    <li><a href='https://keras.io/examples/generative/text_generation_with_miniature_gpt/'>Text Generation With A Miniature GPT</a></li>\n",
        "    <li><a href='https://classroom.github.com/assignment-invitations/db78b5b6e7f6de52f3007f6599db8651/status'>Upload Assignment Here</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793b55d6",
      "metadata": {
        "id": "793b55d6"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7a899af",
      "metadata": {
        "id": "c7a899af"
      },
      "source": [
        "This example demonstrates how to implement an autoregressive language model using a miniature version of the GPT model. The model consists of a single Transformer block with causal masking in its attention layer. We use the text from the IMDB sentiment classification dataset for training and generate new movie reviews for a given prompt. When using this script with your own dataset, make sure it has at least 1 million words.\n",
        "\n",
        "This example should be run with <code>tf-nightly>=2.3.0-dev20200531</code> or with TensorFlow 2.3 or higher.\n",
        "\n",
        "<b>References:</b>\n",
        "<ul>\n",
        "    <li><a href='https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035'>GPT</a></li>\n",
        "    <li><a href='https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe'>GPT-2</a></li>\n",
        "    <li><a href='https://arxiv.org/abs/2005.14165'>GPT-3</a></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cd970a",
      "metadata": {
        "id": "84cd970a"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Setup\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a89927b3",
      "metadata": {
        "id": "a89927b3"
      },
      "outputs": [],
      "source": [
        "# We set the backend to TensorFlow. The code works with\n",
        "# both `tensorflow` and `torch`. It does not work with JAX\n",
        "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
        "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
        "# not support a dynamic `reps` argument.\n",
        "# You can make the code work in JAX by wrapping the\n",
        "# inside of the `causal_attention_mask` function in\n",
        "# a decorator to prevent jit compilation:\n",
        "# `with jax.ensure_compile_time_eval():`.\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import tensorflow\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc72510",
      "metadata": {
        "id": "3fc72510"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Implement Transformer\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "665fbdcb",
      "metadata": {
        "id": "665fbdcb"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = ops.arange(n_dest)[:, None]\n",
        "    j = ops.arange(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = ops.cast(m, dtype)\n",
        "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c707c4",
      "metadata": {
        "id": "56c707c4"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Implement Embedding Layer\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d117bde9",
      "metadata": {
        "id": "d117bde9"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(0, maxlen, 1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3853ba",
      "metadata": {
        "id": "df3853ba"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Implement Mini GPT\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a616308e",
      "metadata": {
        "id": "a616308e"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\",\n",
        "        loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d252f3",
      "metadata": {
        "id": "47d252f3"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Prepare Data\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f4b746",
      "metadata": {
        "id": "09f4b746"
      },
      "source": [
        "Download the IMDB dataset and combine training and validation sets for a text generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "24f5aabc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24f5aabc",
        "outputId": "0643fe16-373d-4956-d46e-330e17ecbcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  5930k      0  0:00:13  0:00:13 --:--:-- 15.2M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "34228bed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34228bed",
        "outputId": "cf3bb480-6770-44d2-d3f6-d8db3cec940c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Changes to speed up runtime:\n",
        "# batch_size 128->16\n",
        "batch_size = 16\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf_data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf_strings.lower(input_string)\n",
        "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tensorflow.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aac9175",
      "metadata": {
        "id": "0aac9175"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Implement Keras Callback for Generating Text\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fecb8b19",
      "metadata": {
        "id": "fecb8b19"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x, verbose=0)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e9a0a4",
      "metadata": {
        "id": "17e9a0a4"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Train the Model\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa2dd6f",
      "metadata": {
        "id": "3fa2dd6f"
      },
      "source": [
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "395af4b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "395af4b1",
        "outputId": "33d2193d-6d68-49e0-ba7a-91448dd9dd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "   3123/Unknown \u001b[1m78s\u001b[0m 21ms/step - loss: 5.4780"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text:\n",
            "this movie is so bad that it is . the first movie is a good idea to be bad , the story , but it will be good , but then the only reason it makes it all the worst acting in all ,\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 23ms/step - loss: 5.4776\n",
            "Epoch 2/2\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.5826generated text:\n",
            "this movie is very good , and it is a very good film ! it is a bit , but i 'm still thinking . it 's about [UNK] \" . [UNK] , the acting is good , the acting , the plot is\n",
            "\n",
            "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 24ms/step - loss: 4.5825\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e5484f52780>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "# Changes to speed up runtime:\n",
        "# verbose 2->1\n",
        "# epochs 25->2\n",
        "model.fit(text_ds, verbose=1, epochs=2, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fbbc82",
      "metadata": {
        "id": "e7fbbc82"
      },
      "source": [
        "<div style='background-color:purple'>\n",
        "\n",
        "## Assignment\n",
        "\n",
        "- [Back to Table of Contents](#table-of-contents)\n",
        "- [Question 1 (Interaction)](#question-1-(interaction))\n",
        "- [Question 2 (Sampling Controls)](#question-2-(sampling-controls))\n",
        "- [Question 3 (Prompt Exploration)](#question-3-(prompt-exploration))\n",
        "- [Question 4 (Analysis and Reflection)](#question-4-(analysis-and-reflection))\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466bd96f",
      "metadata": {
        "id": "466bd96f"
      },
      "source": [
        "## Question 1 (Interaction)\n",
        "\n",
        "- [Back To Assignment Top](#assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e3bd72",
      "metadata": {
        "id": "d5e3bd72"
      },
      "source": [
        "<h4><b>Guidelines:</b></h4>\n",
        "\n",
        "(2 points) You must implement at least one of the following interaction mechanisms:\n",
        "\n",
        "<b>Option A — Function-Based Text Generation (Minimum Requirement)</b>\n",
        "\n",
        "Implement a reusable function (e.g., <code>generate_text(...)</code>) that:\n",
        "<ul>\n",
        "    <li>Accepts a text prompt</li>\n",
        "    <li>Generates a configurable number of new tokens</li>\n",
        "    <li>Uses probabilistic sampling (not argmax)</li>\n",
        "    <li>Returns generated text as a string</li>\n",
        "</ul>\n",
        "You should be able to call it like:\n",
        "\n",
        "<code>generate_text(model, \"this movie is\", num_tokens=60, temperature=0.8, top_k=20)</code>\n",
        "\n",
        "<b>Option B — Interactive CLI Loop (Recommended)</b>\n",
        "\n",
        "Implement a simple <b>command-line or notebook REPL</b> where:\n",
        "<ul>\n",
        "    <li>The user types a prompt</li>\n",
        "    <li>The model generates a continuation</li>\n",
        "    <li>The user can submit multiple prompts in one session</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f76b3f39"
      },
      "source": [
        "def generate_text(model, prompt:str, num_tokens=60, temperature=1, top_k=20):\n",
        "    \"\"\"\n",
        "    Generates text from a trained model given a prompt.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    - model: Keras model trained for text generation.\n",
        "    - prompt(str): The starting prompt for text generation.\n",
        "    - num_tokens(int): The number of new tokens to generate.\n",
        "      - WARNING: This does not affect the output tokens because this function expects a trained model that\n",
        "                 already has a defined token length!  \"maxlen\" is used which is defined in this file to be the\n",
        "                 assigned token length of the trained model\n",
        "    - temperature(float): Controls randomness. Lower = safer, higher = more creative.\n",
        "    - top_k(int): Restricts sampling to the top-k most likely tokens.\n",
        "\n",
        "    Helper Functions\n",
        "    ----------------\n",
        "    - detokenize(token_id)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    - (str): The generated text including the prompt.\n",
        "    \"\"\"\n",
        "    # ----- Helper Functions -----------------------------------------------------\n",
        "    def detokenize(token_id):\n",
        "        return vocab[token_id]\n",
        "\n",
        "\n",
        "\n",
        "    # ----- Tokenize Prompt ------------------------------------------------------\n",
        "    start_tokens = [word_to_index.get(word, 1) for word in prompt.lower().split()]\n",
        "\n",
        "\n",
        "\n",
        "    # ----- Instantiate Variables ------------------------------------------------\n",
        "    generated_tokens = []\n",
        "    current_tokens = list(start_tokens)\n",
        "\n",
        "\n",
        "\n",
        "    # ----- Predict Next Token Iteratively ---------------------------------------\n",
        "    for _ in range(num_tokens):\n",
        "        input_sequence_for_prediction = []\n",
        "        sample_index = len(current_tokens) - 1\n",
        "\n",
        "        if len(current_tokens) < maxlen:\n",
        "            input_sequence_for_prediction = current_tokens + [0] * (maxlen - len(current_tokens))\n",
        "        else:\n",
        "            input_sequence_for_prediction = current_tokens[len(current_tokens) - maxlen:]\n",
        "            sample_index = maxlen - 1\n",
        "\n",
        "        # Make predictions based on input tokens\n",
        "        x = np.array([input_sequence_for_prediction])\n",
        "        y, _ = model.predict(x, verbose=0)\n",
        "        logits = y[0][sample_index]\n",
        "\n",
        "        # Apply temperature to logits\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        top_k_logits, top_k_indices = ops.top_k(logits, k=top_k, sorted=True)\n",
        "        top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "\n",
        "        # Calculate probabilities from the filtered logits\n",
        "        preds = keras.activations.softmax(ops.expand_dims(top_k_logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "\n",
        "        # Sample the next token\n",
        "        sampled_token = np.random.choice(top_k_indices, p=preds)\n",
        "\n",
        "        # Add the sampled token to the sequence of generated tokens and to the current sequence for next prediction\n",
        "        generated_tokens.append(sampled_token)\n",
        "        current_tokens.append(sampled_token)\n",
        "\n",
        "\n",
        "\n",
        "    # ----- Generate Response and Return Response ------------------------------------------------\n",
        "    full_text_tokens = start_tokens + generated_tokens\n",
        "    generated_text = \" \".join([detokenize(t) for t in full_text_tokens])\n",
        "\n",
        "    return generated_text"
      ],
      "id": "f76b3f39",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f988a9cd",
      "metadata": {
        "id": "f988a9cd"
      },
      "source": [
        "---\n",
        "## Question 2 (Sampling Controls)\n",
        "\n",
        "- [Back To Assignment Top](#assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353736e7",
      "metadata": {
        "id": "353736e7"
      },
      "source": [
        "<h4><b>Guidelines:</b></h4>\n",
        "\n",
        "Your interaction must expose <b>at least two</b> of the following parameters:\n",
        "\n",
        "| Parameter          | Description                                                |\n",
        "|--------------------|------------------------------------------------------------|\n",
        "| <b>Temperature</b> | Controls randomness. Lower = safer, higher = more creative |\n",
        "| <b>Top-k</b>       | Restricts sampling to the top-k most likely tokens         |\n",
        "| <b>Max Tokens</b>  | Number of tokens generated beyond the prompt               |\n",
        "\n",
        "You must demonstrate that changing these parameters affects output behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e108cc5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476d518f-0767-43a9-950c-818a0c9f507c"
      },
      "source": [
        "# Default parameters\n",
        "print(\"\\033[35m--- Example 1 (Default) ---\\033[0m\")\n",
        "print(generate_text(model, \"this movie is\"))\n",
        "\n",
        "# Higher temperature\n",
        "print(\"\\033[35m\\n--- Example 2 (Higher Temperature) ---\\033[0m\")\n",
        "print(generate_text(model, \"this movie is\", temperature=1.2))\n",
        "\n",
        "# Lower Top-K\n",
        "print(\"\\033[35m\\n--- Example 3 (Lower Top-K) ---\\033[0m\")\n",
        "print(generate_text(model, \"this movie is\", top_k=5))\n",
        "\n",
        "# More Tokens\n",
        "print(\"\\033[35m\\n--- Example 4 (More Tokens) ---\\033[0m\")\n",
        "print(generate_text(model, \"this movie is\", num_tokens=70))"
      ],
      "id": "e108cc5f",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m--- Example 1 (Default) ---\u001b[0m\n",
            "this movie is very disappointing . it is really about the worst actors . it has no redeeming qualities about this film for this film was in the first 5 minutes in my opinion . it was just a horrible comedy and a movie that is bad , and no , but nothing special about it . . it was just boring and\n",
            "\u001b[35m\n",
            "--- Example 2 (Higher Temperature) ---\u001b[0m\n",
            "this movie is extremely low budget film that i 'm not that the way , this could be forgiven . not quite well written and very well done . there is it 's not . but in a way , so i can 't act and i say that there is the same movie as well . i 'm not sure why people\n",
            "\u001b[35m\n",
            "--- Example 3 (Lower Top-K) ---\u001b[0m\n",
            "this movie is about a [UNK] of mine in the middle of a few times [UNK] \" . but it is not funny and touching . the movie is a bit slow . but i think this movie is a very funny movie . it is a bit of a very entertaining and enjoyable . the acting is not funny . the acting\n",
            "\u001b[35m\n",
            "--- Example 4 (More Tokens) ---\u001b[0m\n",
            "this movie is a little fun , and the movie is awful . i don 't know why it was so bad , it 's so boring . i was looking and the cover . the acting was atrocious . the story was awful , the acting was terrible . the only thing i saw was the worst film of all i could relate , and the story line was the first half\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5bbbabb",
      "metadata": {
        "id": "a5bbbabb"
      },
      "source": [
        "---\n",
        "## Question 3 (Prompt Exploration)\n",
        "\n",
        "- [Back To Assignment Top](#assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f390bc46",
      "metadata": {
        "id": "f390bc46"
      },
      "source": [
        "<h4><b>Guidelines:</b></h4>\n",
        "\n",
        "You must test your model with <b>at least 10 distinct prompts</b>, including:\n",
        "<ul>\n",
        "    <li>Short prompts (2–4 words)</li>\n",
        "    <li>Medium prompts (5–8 words)</li>\n",
        "    <li>At least one ambiguous or incomplete prompt</li>\n",
        "</ul>\n",
        "\n",
        "For each prompt, record:\n",
        "<ul>\n",
        "    <li>Prompt text</li>\n",
        "    <li>Sampling parameters used</li>\n",
        "    <li>Generated output</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d74e04e",
        "outputId": "c0d8a31f-a106-414b-88d6-51df86f7573a"
      },
      "source": [
        "# ----- Short Prompts ------------------------------------------------------------\n",
        "# Break line\n",
        "print(\"\\033[35m\\n--- Short Prompts ---\\033[0m\")\n",
        "\n",
        "# Prompt 1\n",
        "print(\"\\033[33mPrompt: 'in the early', Params: default\\033[0m\")\n",
        "print(generate_text(model, \"in the early\"))\n",
        "\n",
        "# Prompt 2\n",
        "print(\"\\033[33m\\nPrompt: 'column left', Params: temp=0.5, top_k=10\\033[0m\")\n",
        "print(generate_text(model, \"column left\", temperature=0.5, top_k=10))\n",
        "\n",
        "# Prompt 3\n",
        "print(\"\\033[33m\\nPrompt: 'make me', Params: temp=1.0, top_k=30\\033[0m\")\n",
        "print(generate_text(model, \"make me\", temperature=1.0, top_k=30))\n",
        "\n",
        "# Prompt 4\n",
        "print(\"\\033[33m\\nPrompt: 'front leaning', Params: num_tokens=30\\033[0m\")\n",
        "print(generate_text(model, \"front leaning\", num_tokens=30))\n",
        "\n",
        "\n",
        "\n",
        "# ----- Medium Prompts ------------------------------------------------------------\n",
        "# Break line\n",
        "print(\"\\033[35m\\n--- Medium Prompts ---\\033[0m\")\n",
        "\n",
        "# Prompt 1\n",
        "print(\"\\033[33mPrompt: 'the coffee was really bitter', Params: default\\033[0m\")\n",
        "print(generate_text(model, \"the coffee was really bitter\"))\n",
        "\n",
        "# Prompt 2\n",
        "print(\"\\033[33m\\nPrompt: 'traffic in the morning is really bad', Params: temp=0.7, top_k=15\\033[0m\")\n",
        "print(generate_text(model, \"traffic in the morning is really bad\", temperature=0.7, top_k=15))\n",
        "\n",
        "# Prompt 3\n",
        "print(\"\\033[33m\\nPrompt: 'slow is smooth and smooth is', Params: num_tokens=70\\033[0m\")\n",
        "print(generate_text(model, \"slow is smooth and smooth is\", num_tokens=70))\n",
        "\n",
        "# Prompt 4\n",
        "print(\"\\033[33m\\nPrompt: 'where did all the snow go in', Params: temp=1.1, top_k=25\\033[0m\")\n",
        "print(generate_text(model, \"where did all the snow go in\", temperature=1.1, top_k=25))\n",
        "\n",
        "\n",
        "\n",
        "# ----- Ambiguous or Incomplete Prompts ------------------------------------------\n",
        "# Break line\n",
        "print(\"\\033[35m\\n--- Ambiguous/Incomplete Prompts ---\\033[0m\")\n",
        "\n",
        "# Prompt 1\n",
        "print(\"\\033[33mPrompt: 'tell me what you want what you really really want', Params: default\\033[0m\")\n",
        "print(generate_text(model, \"tell me what you want what you really really want\"))\n",
        "\n",
        "# Prompt 2\n",
        "print(\"\\033[33m\\nPrompt: 'I am', Params: temp=0.9, top_k=20\\033[0m\")\n",
        "print(generate_text(model, \"I am\", temperature=0.9, top_k=20))"
      ],
      "id": "2d74e04e",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[35m\n",
            "--- Short Prompts ---\u001b[0m\n",
            "\u001b[33mPrompt: 'in the early', Params: default\u001b[0m\n",
            "in the early film , i enjoyed this film - i have to say that it was bad . however [UNK] is pretty good , but , the acting was bad , even the [UNK] of the movie was bad . i was expecting a [UNK] ' type of [UNK] for [UNK] [UNK] , [UNK] in [UNK] and [UNK] was a bit of\n",
            "\u001b[33m\n",
            "Prompt: 'column left', Params: temp=0.5, top_k=10\u001b[0m\n",
            "column left me with the [UNK] of [UNK] \" and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] . \" is the movie [UNK] [UNK] \" of the [UNK] [UNK] [UNK] \" . it is a [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ) . [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
            "\u001b[33m\n",
            "Prompt: 'make me', Params: temp=1.0, top_k=30\u001b[0m\n",
            "make me honest about this film [UNK] and the title [UNK] \" is a [UNK] of the [UNK] [UNK] \" show . however , a [UNK] in its [UNK] way : the character is a bit to watch as a [UNK] comedy than [UNK] \" . one of the worst movies i have seen in my life . . it was the\n",
            "\u001b[33m\n",
            "Prompt: 'front leaning', Params: num_tokens=30\u001b[0m\n",
            "front leaning is about the movie . the plot is about a couple is the [UNK] [UNK] of the [UNK] \" ) who is given the movie . i have been [UNK]\n",
            "\u001b[35m\n",
            "--- Medium Prompts ---\u001b[0m\n",
            "\u001b[33mPrompt: 'the coffee was really bitter', Params: default\u001b[0m\n",
            "the coffee was really bitter , that i like [UNK] [UNK] the new world \" - that is a great , but not so many of the most popular characters imaginable , but no more , [UNK] and a half -human [UNK] \" . in a few more serious attempts to make it a mockery of any more than a serious [UNK] . i 'm\n",
            "\u001b[33m\n",
            "Prompt: 'traffic in the morning is really bad', Params: temp=0.7, top_k=15\u001b[0m\n",
            "traffic in the morning is really bad , a bad film , with a few good moments . . . . . the movie has to be very good , and if you are a little bit of gore , and then it 's not a good movie , but the movie just isn 't . it 's a lot like this . the only one .\n",
            "\u001b[33m\n",
            "Prompt: 'slow is smooth and smooth is', Params: num_tokens=70\u001b[0m\n",
            "slow is smooth and smooth is not about . the end of the [UNK] between and [UNK] [UNK] . i don 't know why did these kids are not to have a little [UNK] \" in [UNK] in order to be [UNK] . the movie is a movie that has not the worst movie making . it really doesn 't get [UNK] \" or [UNK] to the movie ) that are . the story is the\n",
            "\u001b[33m\n",
            "Prompt: 'where did all the snow go in', Params: temp=1.1, top_k=25\u001b[0m\n",
            "where did all the snow go in front of a new show and was on abc broadcast today . it seems to be a tv series of other tv series : it wasn 't as good news or not , though and i was intrigued by a friend and i 've been very disappointed as a [UNK] story that has been in a way . i remember\n",
            "\u001b[35m\n",
            "--- Ambiguous/Incomplete Prompts ---\u001b[0m\n",
            "\u001b[33mPrompt: 'tell me what you want what you really really want', Params: default\u001b[0m\n",
            "tell me what you want what you really really want . that 's a real movie . but it certainly isn 't the worst movie of all . the actors have been great and so good that it 's not funny or anything . i know what you have to see ? ? ! ! ? ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\u001b[33m\n",
            "Prompt: 'I am', Params: temp=0.9, top_k=20\u001b[0m\n",
            "i am of the first few minutes of this film is not funny but i 've seen it again and i have seen the first few seasons so i have to say that this is the [UNK] \" and i think the show is a bit of a little too bad to watch , and it is a [UNK] that will take\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9756b1",
      "metadata": {
        "id": "af9756b1"
      },
      "source": [
        "---\n",
        "## Question 4 (Analysis and Reflection)\n",
        "\n",
        "- [Back To Assignment Top](#assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42ecc96",
      "metadata": {
        "id": "c42ecc96"
      },
      "source": [
        "<h4><b>Guidelines:</b></h4>\n",
        "\n",
        "In your submission, include a short written analysis addressing:\n",
        "\n",
        "A. (6 points) Sampling Behavior\n",
        "<ul>\n",
        "    <li>(2 points) How does increasing <b>temperature</b> affect coherence?</li>\n",
        "    <li>(2 points) How does changing <b>top-k</b> affect repetition or diversity?</li>\n",
        "    <li>(2 points) Which settings produced the “best” outputs, and why?</li>\n",
        "\n",
        "B. (10 points) Model Limitations\n",
        "\n",
        "(8 points) Identify <b>at least one failure mode</b>, such as:\n",
        "<ul>\n",
        "    <li>Repetitive loops</li>\n",
        "    <li>Loss of grammatical structure</li>\n",
        "    <li>Sudden topic drift</li>\n",
        "    <li>Nonsensical phrasing</li>\n",
        "</ul>\n",
        "\n",
        "(2 points) Explain why this happens in a <b>small, single-block GPT model</b>.\n",
        "\n",
        "C. (6 points) Architectural Reflection\n",
        "\n",
        "Briefly discuss how model size and training data limitations affect:\n",
        "<ul>\n",
        "    <li>(2 points) Long-range coherence</li>\n",
        "    <li>(2 points) Semantic consistency</li>\n",
        "    <li>(2 points) Real-world usability</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A\n",
        "  - Increasing temperature affects coherence by allowing predicted tokens to be further away from the central cluster (I would assume), alternatively, it allows for more creative responses\n",
        "  - Lower Top-K seems to affect more repetitive answers whereas higher creates more non-sensical/contradictory outputs\n",
        "  - It appears that keeping Top-K around 20 and temperature above 1 seemed to produce decent results, the tokens is merely just how long of a response I want\n",
        "- B\n",
        "  - As mentioned earlier, moving the Top-K too high appears to create a lot of nonsensical/contradictory statements in the response\n",
        "  - This likely happens because the single-block is working very linearly and letting Top-K not cross-reference the best word out of multiple pools may start to feel random in response\n",
        "- C\n",
        "  - More training data and larger model size in general should increase long-range coherence, semantic consistency, and real-world usability simply because its determined probabilities should be better refined and more accurate to the true prediction; however, for obvious reasons, this will cost more.  Potentially why all the AI-based prompt services have a free version to allow for more information and data to train off of for future models"
      ],
      "metadata": {
        "id": "UwG0Y761PZCB"
      },
      "id": "UwG0Y761PZCB"
    },
    {
      "cell_type": "markdown",
      "id": "34cd00db",
      "metadata": {
        "id": "34cd00db"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}