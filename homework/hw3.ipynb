{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c6c6b",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Setup](#setup)\n",
    "- [Implement Transformer](#implement-transformer)\n",
    "- [Implement Embedding Layer](#implement-embedding-layer)\n",
    "- [Implement Mini GPT](#implement-mini-gpt)\n",
    "- [Prepare Data](#prepare-data)\n",
    "- [Implement Keras Callback for Generating Text](#implement-keras-callback-for-generating-text)\n",
    "- [Train the Model](#train-the-model)\n",
    "- [Assignment](#assignment)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadac36",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><a href='https://keras.io/examples/generative/text_generation_with_miniature_gpt/'>Text Generation With A Miniature GPT</a></li>\n",
    "    <li><a href='https://classroom.github.com/assignment-invitations/db78b5b6e7f6de52f3007f6599db8651/status'>Upload Assignment Here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b55d6",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a899af",
   "metadata": {},
   "source": [
    "This example demonstrates how to implement an autoregressive language model using a miniature version of the GPT model. The model consists of a single Transformer block with causal masking in its attention layer. We use the text from the IMDB sentiment classification dataset for training and generate new movie reviews for a given prompt. When using this script with your own dataset, make sure it has at least 1 million words.\n",
    "\n",
    "This example should be run with <code>tf-nightly>=2.3.0-dev20200531</code> or with TensorFlow 2.3 or higher.\n",
    "\n",
    "<b>References:</b>\n",
    "<ul>\n",
    "    <li><a href='https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035'>GPT</a></li>\n",
    "    <li><a href='https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe'>GPT-2</a></li>\n",
    "    <li><a href='https://arxiv.org/abs/2005.14165'>GPT-3</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd970a",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Setup\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89927b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:27:27.105846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# We set the backend to TensorFlow. The code works with\n",
    "# both `tensorflow` and `torch`. It does not work with JAX\n",
    "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
    "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
    "# not support a dynamic `reps` argument.\n",
    "# You can make the code work in JAX by wrapping the\n",
    "# inside of the `causal_attention_mask` function in\n",
    "# a decorator to prevent jit compilation:\n",
    "# `with jax.ensure_compile_time_eval():`.\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72510",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Implement Transformer\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665fbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c707c4",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Implement Embedding Layer\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d117bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3853ba",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Implement Mini GPT\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a616308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d252f3",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b746",
   "metadata": {},
   "source": [
    "Download the IMDB dataset and combine training and validation sets for a text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f5aabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  4607k      0  0:00:17  0:00:17 --:--:-- 6366k4051k      0  0:00:20  0:00:14  0:00:06 4422k 0     0  4138k      0  0:00:19  0:00:15  0:00:04 4643k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34228bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 11:29:39.386504: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tensorflow.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac9175",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Implement Keras Callback for Generating Text\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecb8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9a0a4",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Train the Model\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2dd6f",
   "metadata": {},
   "source": [
    "Note: This code should preferably be run on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395af4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/COMP3703/lib/python3.11/site-packages/keras/src/layers/layer.py:1505: UserWarning: Layer 'transformer_block_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.''\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/COMP3703/lib/python3.11/site-packages/keras/src/layers/layer.py:424: UserWarning: `build()` was called on layer 'transformer_block_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling TransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_block_1' (of type TransformerBlock). Either the `TransformerBlock.call()` method is incorrect, or you need to implement the `TransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 80, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_5>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperatorNotAllowedInGraphError\u001b[39m            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.fit(text_ds, verbose=\u001b[32m2\u001b[39m, epochs=\u001b[32m25\u001b[39m, callbacks=[text_gen_callback])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m x = embedding_layer(inputs)\n\u001b[32m     12\u001b[39m transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m x = \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m outputs = layers.Dense(vocab_size)(x)\n\u001b[32m     15\u001b[39m model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/COMP3703/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mTransformerBlock.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     35\u001b[39m batch_size = input_shape[\u001b[32m0\u001b[39m]\n\u001b[32m     36\u001b[39m seq_len = input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m causal_mask = \u001b[43mcausal_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbool\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m attention_output = \u001b[38;5;28mself\u001b[39m.att(inputs, inputs, attention_mask=causal_mask)\n\u001b[32m     39\u001b[39m attention_output = \u001b[38;5;28mself\u001b[39m.dropout1(attention_output)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcausal_attention_mask\u001b[39m\u001b[34m(batch_size, n_dest, n_src, dtype)\u001b[39m\n\u001b[32m     11\u001b[39m mask = ops.reshape(mask, [\u001b[32m1\u001b[39m, n_dest, n_src])\n\u001b[32m     12\u001b[39m mult = ops.concatenate(\n\u001b[32m     13\u001b[39m     [ops.expand_dims(batch_size, -\u001b[32m1\u001b[39m), ops.convert_to_tensor([\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m])], \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOperatorNotAllowedInGraphError\u001b[39m: Exception encountered when calling TransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_block_1' (of type TransformerBlock). Either the `TransformerBlock.call()` method is incorrect, or you need to implement the `TransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by TransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 80, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_5>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fbbc82",
   "metadata": {},
   "source": [
    "<div style='background-color:purple'>\n",
    "\n",
    "## Assignment\n",
    "\n",
    "- [Back to Table of Contents](#table-of-contents)\n",
    "- [Question 1 (Interaction)](#question-1-(interaction))\n",
    "- [Question 2 (Sampling Controls)](#question-2-(sampling-controls))\n",
    "- [Question 3 (Prompt Exploration)](#question-3-(prompt-exploration))\n",
    "- [Question 4 (Analysis and Reflection)](#question-4-(analysis-and-reflection))\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bd96f",
   "metadata": {},
   "source": [
    "## Question 1 (Interaction)\n",
    "\n",
    "- [Back To Assignment Top](#assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3bd72",
   "metadata": {},
   "source": [
    "<h4><b>Guidelines:</b></h4>\n",
    "\n",
    "(2 points) You must implement at least one of the following interaction mechanisms:\n",
    "\n",
    "<b>Option A — Function-Based Text Generation (Minimum Requirement)</b>\n",
    "\n",
    "Implement a reusable function (e.g., <code>generate_text(...)</code>) that:\n",
    "<ul>\n",
    "    <li>Accepts a text prompt</li>\n",
    "    <li>Generates a configurable number of new tokens</li>\n",
    "    <li>Uses probabilistic sampling (not argmax)</li>\n",
    "    <li>Returns generated text as a string</li>\n",
    "</ul>\n",
    "You should be able to call it like:\n",
    "\n",
    "<code>generate_text(model, \"this movie is\", num_tokens=60, temperature=0.8, top_k=20)</code>\n",
    "\n",
    "<b>Option B — Interactive CLI Loop (Recommended)</b>\n",
    "\n",
    "Implement a simple <b>command-line or notebook REPL</b> where:\n",
    "<ul>\n",
    "    <li>The user types a prompt</li>\n",
    "    <li>The model generates a continuation</li>\n",
    "    <li>The user can submit multiple prompts in one session</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988a9cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 2 (Sampling Controls)\n",
    "\n",
    "- [Back To Assignment Top](#assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353736e7",
   "metadata": {},
   "source": [
    "<h4><b>Guidelines:</b></h4>\n",
    "\n",
    "Your interaction must expose <b>at least two</b> of the following parameters:\n",
    "\n",
    "| Parameter          | Description                                                |\n",
    "|--------------------|------------------------------------------------------------|\n",
    "| <b>Temperature</b> | Controls randomness. Lower = safer, higher = more creative |\n",
    "| <b>Top-k</b>       | Restricts sampling to the top-k most likely tokens         |\n",
    "| <b>Max Tokens</b>  | Number of tokens generated beyond the prompt               |\n",
    "\n",
    "You must demonstrate that changing these parameters affects output behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbbabb",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 3 (Prompt Exploration)\n",
    "\n",
    "- [Back To Assignment Top](#assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390bc46",
   "metadata": {},
   "source": [
    "<h4><b>Guidelines:</b></h4>\n",
    "\n",
    "You must test your model with <b>at least 10 distinct prompts</b>, including:\n",
    "<ul>\n",
    "    <li>Short prompts (2–4 words)</li>\n",
    "    <li>Medium prompts (5–8 words)</li>\n",
    "    <li>At least one ambiguous or incomplete prompt</li>\n",
    "</ul>\n",
    " \n",
    "For each prompt, record:\n",
    "<ul>\n",
    "    <li>Prompt text</li>\n",
    "    <li>Sampling parameters used</li>\n",
    "    <li>Generated output</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9756b1",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 4 (Analysis and Reflection)\n",
    "\n",
    "- [Back To Assignment Top](#assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ecc96",
   "metadata": {},
   "source": [
    "<h4><b>Guidelines:</b></h4>\n",
    "\n",
    "In your submission, include a short written analysis addressing:\n",
    "\n",
    "A. (6 points) Sampling Behavior\n",
    "<ul>\n",
    "    <li>(2 points) How does increasing <b>temperature</b> affect coherence?</li>\n",
    "    <li>(2 points) How does changing <b>top-k</b> affect repetition or diversity?</li>\n",
    "    <li>(2 points) Which settings produced the “best” outputs, and why?</li>\n",
    "\n",
    "B. (10 points) Model Limitations\n",
    "\n",
    "(8 points) Identify <b>at least one failure mode</b>, such as:\n",
    "<ul>\n",
    "    <li>Repetitive loops</li>\n",
    "    <li>Loss of grammatical structure</li>\n",
    "    <li>Sudden topic drift</li>\n",
    "    <li>Nonsensical phrasing</li>\n",
    "</ul>\n",
    "\n",
    "(2 points) Explain why this happens in a <b>small, single-block GPT model</b>.\n",
    "\n",
    "C. (6 points) Architectural Reflection\n",
    "\n",
    "Briefly discuss how model size and training data limitations affect:\n",
    "<ul>\n",
    "    <li>(2 points) Long-range coherence</li>\n",
    "    <li>(2 points) Semantic consistency</li>\n",
    "    <li>(2 points) Real-world usability</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd00db",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP3703",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
